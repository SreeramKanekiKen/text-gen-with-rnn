{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file=tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(f\"Length of text: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f\"{len(vocab)} unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text=[\"abcdefg\", \"xyz\"]\n",
    "chars=tf.strings.unicode_split(example_text, input_encoding=\"UTF-8\")\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars=tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids=tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars=chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids=ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_Dataset=tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_Dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=100\n",
    "examples_per_epoch=len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences=ids_Dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(1):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text=sequence[:-1]\n",
    "    target_text=sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE=64\n",
    "BUFFER_SIZE=10000\n",
    "\n",
    "dataset=(\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab)\n",
    "\n",
    "embedding_dim=256\n",
    "\n",
    "rnn_units=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru=tf.keras.layers.GRU(\n",
    "            rnn_units, return_sequences=True, return_state=True\n",
    "        )\n",
    "        self.dense=tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x=self.embedding(inputs, training=training)\n",
    "        if states is None:\n",
    "            states=self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x=self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions=model(input_example_batch)\n",
    "    print(\n",
    "        example_batch_predictions.shape,\n",
    "        \"# (batch_size, sequence_length, vocab_size)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices=tf.random.categorical(\n",
    "    example_batch_predictions[0], num_samples=1\n",
    ")\n",
    "sampled_indices=tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 23, 27, 56, 36, 16, 19,  7,  2, 10, 26, 23, 17, 30, 29, 62, 42,\n",
       "       20, 44, 54,  9, 10, 51, 36, 20, 28, 19,  6, 36, 22,  4, 61, 20, 64,\n",
       "       45, 64, 47, 54, 26, 60, 62, 39, 47,  4, 41,  3, 53, 30, 18, 27, 25,\n",
       "       39,  5,  5, 45, 20, 61, 32, 33, 33, 33, 57, 43, 32, 17, 25, 22, 16,\n",
       "        7, 35, 32, 36, 35, 58, 13,  2, 49, 40,  6, 18, 22, 15, 39, 36, 47,\n",
       "       10, 26, 56, 28, 26, 44, 32, 12, 65, 43, 15,  0, 13, 16, 39],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b', resides this dejected Mariana. At that\\nplace call upon me; and dispatch with Angelo, that\\nit may b'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"QJNqWCF, 3MJDQPwcGeo.3lWGOF'WI$vGyfyhoMuwZh$b!nQENLZ&&fGvSTTTrdSDLIC,VSWVs? ja'EIBZWh3MqOMeS;zdB[UNK]?CZ\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape:  (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:      tf.Tensor(4.1891365, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss=loss(target_example_batch, example_batch_predictions)\n",
    "print(\n",
    "    \"Prediction Shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \"# (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "print(\"Mean loss:     \",example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.965805"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir=\"./training_checkpoints\"\n",
    "checkpoint_prefix=os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 20s 109ms/step - loss: 1.1554\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 20s 108ms/step - loss: 1.1137\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 20s 110ms/step - loss: 1.0687\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 20s 110ms/step - loss: 1.0218\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 20s 109ms/step - loss: 0.9723\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 20s 110ms/step - loss: 0.9194\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 20s 109ms/step - loss: 0.8663\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 20s 110ms/step - loss: 0.8135\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 21s 112ms/step - loss: 0.7620\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 20s 111ms/step - loss: 0.7141\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 20s 111ms/step - loss: 0.6712\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 21s 112ms/step - loss: 0.6318\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 20s 110ms/step - loss: 0.5972\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 21s 111ms/step - loss: 0.5674\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 21s 111ms/step - loss: 0.5424\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 22s 119ms/step - loss: 0.5223\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 22s 119ms/step - loss: 0.5065\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 22s 122ms/step - loss: 0.4911\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 22s 122ms/step - loss: 0.4759\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 23s 125ms/step - loss: 0.4652\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 22s 122ms/step - loss: 0.4542\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 23s 127ms/step - loss: 0.4487\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 23s 127ms/step - loss: 0.4452\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 24s 132ms/step - loss: 0.4391\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 24s 132ms/step - loss: 0.4292\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 24s 133ms/step - loss: 0.4293\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 24s 133ms/step - loss: 0.4306\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 24s 130ms/step - loss: 0.4222\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 24s 133ms/step - loss: 0.4193\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 24s 133ms/step - loss: 0.4183\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "history=model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature=temperature\n",
    "        self.model=model\n",
    "        self.chars_from_ids=chars_from_ids\n",
    "        self.ids_from_chars=ids_from_chars\n",
    "        \n",
    "        skip_ids=self.ids_from_chars(['[UNK]'])[:,None]\n",
    "        sparse_mask=tf.SparseTensor(\n",
    "            values=[-float(\"inf\")]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
    "        )\n",
    "        \n",
    "        self.prediction_mask=tf.sparse.to_dense(sparse_mask)\n",
    "        \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        input_chars=tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids=self.ids_from_chars(input_chars).to_tensor()\n",
    "        \n",
    "        predicted_logits, states=self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "        \n",
    "        predicted_logits=predicted_logits[:, -1, :]\n",
    "        predicted_logits=predicted_logits/self.temperature\n",
    "        predicted_logits=predicted_logits+self.prediction_mask\n",
    "        \n",
    "        predicted_ids=tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids=tf.squeeze(predicted_ids, axis=-1)\n",
    "        \n",
    "        predicted_chars=self.chars_from_ids(predicted_ids)\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model=OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The excepprog's man is a pitician of great\n",
      "But doth become my wife in his grace.\n",
      "\n",
      "First Gentleman:\n",
      "Well, my lord: welcome, gladuous do answer there.\n",
      "\n",
      "LUCIO:\n",
      "Good Bolingbrok,\n",
      "So long a-monity will be bruitful victory.\n",
      "Is not the queen who stand now with rain's ta'en?\n",
      "\n",
      "First Servant:\n",
      "What's honest tuated shall be that of Christen of the earth,\n",
      "The generous and unperitice.\n",
      "What, ho, how but we omit,\n",
      "When thou hast have tress'd me of this defend.\n",
      "I will, my noble cousin; from him proud, and\n",
      "would pluck him conceallied accusans: God, which I please,\n",
      "Unworthy thee, fond worthy widows,\n",
      "And make me in his master, but through the world so side;\n",
      "For their advices, tirrahts with wolves are of such\n",
      "deeps, from out a world as much belosed:\n",
      "More than our faults, I hold thee better:\n",
      "By heaven, I fear, not for my company.\n",
      "\n",
      "LADY GREY:\n",
      "No, man; son, and my love.\n",
      "When this is lost a body for battle;\n",
      "And here I take them such a sin.\n",
      "\n",
      "KING HENRY VI:\n",
      "I swear I'll crave the brother of your lasting where;\n",
      "Th \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.612188339233398\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Ay, as your reigning men have done unto his\n",
      "prayers. O, the king's son,\n",
      "Who, with a grandar kingly charities that I was not queen.\n",
      "\n",
      "PETRUCHIO:\n",
      "I say it is remembar.\n",
      "\n",
      "GLOUCESTER:\n",
      "Come now, come by: almost all these powers\n",
      "Apoperity on our heart: these are the spoilest\n",
      "so vouchish part off together to the day,\n",
      "So would have setted: hanging in the trests ones, you will te\n",
      "dagger-in-lew-ply of thy scaleful wedding-day.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Lord marshal, let thee jot! Come you to some hand:\n",
      "What if become my life upon this hother,\n",
      "Not your consent, if you should not be avoided,\n",
      "Stand by unlikely turn to Henry.\n",
      "Please you to do't at place?\n",
      "\n",
      "JULIET:\n",
      "Thy body shall be placed in their death at gate\n",
      "And use your manners disperse that bawdly change,\n",
      "so.' farewell, and still too good father, bring it there.\n",
      "\n",
      "ROMEO:\n",
      "Gream of Naples, pity'd him, the sons of oathful evils,\n",
      "That is not made of Angelo. if my resolve\n",
      "I never yet make men earth.\n",
      "High-selves---beat not at all as yonder already:\n",
      "My unillos \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.6041553020477295\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
